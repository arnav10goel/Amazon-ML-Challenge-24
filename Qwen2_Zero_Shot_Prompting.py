# Importing the required libraries
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch
import pandas as pd

def prompt_design(image_path, entity_name, entity_unit_map):
    """
    Creates a prompt for extracting a numerical value and its unit from an image.
    
    Parameters:
    - image_path (str): The path to the image file containing the entity.
    - entity_name (str): The name of the entity to be extracted (e.g., 'weight').
    - entity_unit_map (dict): A dictionary mapping entity names to a list of plausible units.
    
    Returns:
    - messages (list): A list of dictionaries formatted to be used as a prompt for a model to extract information.
    """
    
    # Retrieve the list of plausible units for the given entity name
    plausible_units = entity_unit_map[entity_name]

    # Create a string of plausible units separated by commas
    str_units = ""
    for unit in plausible_units:
        str_units += unit + ", "

    # Create the prompt string, specifying how the information should be formatted in the response
    prompt = f'''Please extract the item weight and its unit of measurement from the image, providing them separately. Ensure that the unit is one of the following: {str_units}. Format your response as follows:
    \nValue: <only the numerical value>
    \nUnit: <unit of measurement from the specified list>'''

    # Construct the message to be sent, containing both the image and the prompt text
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image_path,  # Embed the image path
                },
                {
                    "type": "text", 
                    "text": prompt  # Include the prompt text
                },
            ],
        }
    ]

    # Return the constructed message
    return messages

def text(messages, model):
    """
    Processes a list of messages, prepares them for model inference, and generates text output.

    Parameters:
    - messages (list): A list of message dictionaries containing text and possibly image or video inputs.
    - model (transformer model): The model used to generate text output.

    Returns:
    - output_text (list): A list of strings generated by the model for each input message.
    """
    
    # Prepare text input for the model by applying a chat template to each message.
    # This includes any tokenization or additional prompts required for generation.
    texts = [
        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
        for msg in messages
    ]
    
    # Process images and videos from the messages if available.
    image_inputs, video_inputs = process_vision_info(messages)
    
    # Prepare the inputs for the model, including text, images, and videos.
    # The processor handles tokenization, padding, and conversion to tensors.
    inputs = processor(
        text=texts,
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",  # Return the input as PyTorch tensors.
    )

    # Generate output text using the model.
    # 'max_new_tokens' limits the length of the generated output.
    generated_ids = model.generate(**inputs, max_new_tokens=128)
    
    # Trim the generated output to remove input tokens, keeping only the newly generated tokens.
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    
    # Decode the generated tokens into text, skipping special tokens and preserving spaces.
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )
    
    # Return the final output text generated by the model.
    return output_text


def process_csv(file_path, entity_unit_map, batch, model):
    """
    Processes a CSV file containing image links and entity names, applies a model to extract information,
    and stores the results back into the CSV file.

    Parameters:
    - file_path (str): Path to the input CSV file containing image links and entity names.
    - entity_unit_map (dict): A dictionary mapping entity names to a list of plausible units.
    - batch (int): The number of rows to process in each batch.
    - model (transformer model): The model used to generate text output from the input data.

    Returns:
    - None: The function saves the processed data into a new CSV file.
    """
    
    # Read the input CSV file into a DataFrame.
    df = pd.read_csv(file_path)
    
    # Add a new column 'answer' to store the model's output for each row.
    df['answer'] = None 

    # Process the DataFrame in batches to handle large data efficiently.
    for start in range(0, len(df), batch):
        end = start + batch
        # Select the current batch of rows to process.
        batch_df = df.iloc[start:end]
        
        messages = []  # List to store prompts for the current batch.
        
        # Iterate over each row in the current batch.
        for index, row in batch_df.iterrows():
            # Construct the image path using the base path and the image filename from the 'image_link' column.
            image_path = "../test_images/" + row['image_link'].split('/')[-1]
            
            # Create the prompt for the current row using 'prompt_design' function.
            prompt = prompt_design(image_path, row['entity_name'], entity_unit_map)
            
            # Append the constructed prompt to the messages list.
            messages.append(prompt)
        
        # Generate text output using the model for the batch of messages.
        output_text = text(messages, model)
        
        # Store the model's output in the 'answer' column for the current batch.
        df.loc[start:end-1, 'answer'] = output_text
        
        # Print the output for debugging or monitoring progress.
        print(output_text, start, end)
    
    # Save the updated DataFrame to a new CSV file.
    df.to_csv('../ted_test.csv', index=False)



model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
    device_map="auto",
)

min_pixels = 256*28*28
max_pixels = 1280*28*28
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)

entity_unit_map = {
    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},
    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},
    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},
    'item_weight': {'gram',
        'kilogram',
        'microgram',
        'milligram',
        'ounce',
        'pound',
        'ton'},
    'maximum_weight_recommendation': {'gram',
        'kilogram',
        'microgram',
        'milligram',
        'ounce',
        'pound',
        'ton'},
    'voltage': {'kilovolt', 'millivolt', 'volt'},
    'wattage': {'kilowatt', 'watt'},
    'item_volume': {'centilitre',
        'cubic foot',
        'cubic inch',
        'cup',
        'decilitre',
        'fluid ounce',
        'gallon',
        'imperial gallon',
        'litre',
        'microlitre',
        'millilitre',
        'pint',
        'quart'}
}

test_file_path = "../test.csv"

process_csv(test_file_path, entity_unit_map, 16, model)